{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "597199f3-266b-454f-a4e1-921e61e4b59d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953b440f-6182-45a3-b4ed-c86693b36ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f83c5e2-21e1-4243-8eb1-ff8f1284e073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "30edb4c9-e5a2-4822-ba14-529123240bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Bob.', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 35, 'total_tokens': 40}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8868f881-de3f-4c56-b13d-10efad38d468-0', usage_metadata={'input_tokens': 35, 'output_tokens': 5, 'total_tokens': 40})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba229c44-a4a3-4be9-8683-3e616b9a3ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2178e4b1-4725-42e7-8948-0dcbad1f2d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "31cbbaca-ea63-4be0-865d-eb1e642cf3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Bob! How can I assist you today?'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Bob\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad27241a-82ef-4858-a86b-0b6abd688b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Bob.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c0bc5141-ed72-4932-9417-e3070007eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eefafb4b-3515-4a1f-8122-b7f6d3ba9ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "601ac677-37c9-4c42-876f-b0c1e964bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "47afaf33-4ff6-4431-a3ae-704b23e49918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, Jim! How can I assist you today?'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Jim\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5ba58827-e0fb-4c75-b645-7950ab5b02cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Jim. How can I help you, Jim?'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b367a4b-7feb-4ded-bf21-8b15a16b7a08",
   "metadata": {},
   "source": [
    "## Making a Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "723c3f7c-15d9-4961-b99b-2b3f3318df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2f20c7ef-4d3d-4c08-a491-814b310087af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b4ffce30-ae15-44c1-9835-6998ad0f687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc11\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b578169-b578-4b5e-8ecc-b262ab8a306f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola Todd! ¿En qué puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! I'm todd\")], \"language\": \"Spanish\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "08572990-7bb3-4665-9093-54ac5140817e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tu nombre es Todd. ¿Hay algo más en lo que pueda ayudarte hoy?'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"whats my name?\")], \"language\": \"Spanish\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d001612-a1f2-401b-b372-51413170b5ef",
   "metadata": {},
   "source": [
    "## Managing Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea9fd803-c426-4069-b43c-ab9d6edee094",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "19d61230-621a-4f25-9b20-995f6f061fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\"),\n",
       " HumanMessage(content='whats 2 + 2'),\n",
       " AIMessage(content='4'),\n",
       " HumanMessage(content='thanks'),\n",
       " AIMessage(content='no problem!'),\n",
       " HumanMessage(content='having fun?'),\n",
       " AIMessage(content='yes!')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7f4e9e01-dc30-472a-8c0a-37687faca50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked \"what\\'s 2 + 2\"'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc758e10-482a-40cf-bf2f-cd2ca1c392af",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ad5a612b-2dd3-4b24-959d-9c2225b42210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Hi| Todd|!| Sure|,| here|'s| a| joke| for| you|:\n",
      "\n",
      "|Why| don|'t| scientists| trust| atoms|?\n",
      "\n",
      "|Because| they| make| up| everything|!||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc15\"}}\n",
    "for r in with_message_history.stream(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"hi! I'm todd. tell me a joke\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    "):\n",
    "    print(r.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384c250a-5dc8-4b2f-ac8b-4145e80ccf0a",
   "metadata": {},
   "source": [
    "## Conversational History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bddbca5f-1349-4afe-85e3-cbce53a197ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-chroma bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a76b5f6-adb9-4017-b511-71e0569a018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load, chunk and index the contents of the blog to create a retriever.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f2fa154-7c77-4956-aee2-f3b65d7cadbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Incorporate the retriever into a question-answering chain.\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8694aa89-43a2-4017-83a8-69b3dbe730c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. This method helps agents or models handle challenging tasks by dividing them into more manageable subtasks. Task decomposition can be achieved through prompting techniques like Chain of Thought (CoT) or Tree of Thoughts to guide the model in step-by-step thinking and reasoning processes.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is Task Decomposition?\"})\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb3902bb-ffbb-41e9-8719-aa65a065cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0852f725-7a4a-4b99-b089-ae1ef42d9c5d",
   "metadata": {},
   "source": [
    "create_stuff_documents_chain: Focuses on formatting and processing a set of documents with a language model.\n",
    "\n",
    "create_retrieval_chain: Focuses on retrieving relevant documents based on an input query and then processing these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e4f8597-69d4-4dc4-b13e-dc4e534ff083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mcreate_retrieval_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mretriever\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Union[BaseRetriever, Runnable[dict, RetrieverOutput]]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcombine_docs_chain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Runnable[Dict[str, Any], str]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'Runnable'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Create retrieval chain that retrieves documents and then passes them on.\n",
       "\n",
       "Args:\n",
       "    retriever: Retriever-like object that returns list of documents. Should\n",
       "        either be a subclass of BaseRetriever or a Runnable that returns\n",
       "        a list of documents. If a subclass of BaseRetriever, then it\n",
       "        is expected that an `input` key be passed in - this is what\n",
       "        is will be used to pass into the retriever. If this is NOT a\n",
       "        subclass of BaseRetriever, then all the inputs will be passed\n",
       "        into this runnable, meaning that runnable should take a dictionary\n",
       "        as input.\n",
       "    combine_docs_chain: Runnable that takes inputs and produces a string output.\n",
       "        The inputs to this will be any original inputs to this chain, a new\n",
       "        context key with the retrieved documents, and chat_history (if not present\n",
       "        in the inputs) with a value of `[]` (to easily enable conversational\n",
       "        retrieval.\n",
       "\n",
       "Returns:\n",
       "    An LCEL Runnable. The Runnable return is a dictionary containing at the very\n",
       "    least a `context` and `answer` key.\n",
       "\n",
       "Example:\n",
       "    .. code-block:: python\n",
       "\n",
       "        # pip install -U langchain langchain-community\n",
       "\n",
       "        from langchain_community.chat_models import ChatOpenAI\n",
       "        from langchain.chains.combine_documents import create_stuff_documents_chain\n",
       "        from langchain.chains import create_retrieval_chain\n",
       "        from langchain import hub\n",
       "\n",
       "        retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
       "        llm = ChatOpenAI()\n",
       "        retriever = ...\n",
       "        combine_docs_chain = create_stuff_documents_chain(\n",
       "            llm, retrieval_qa_chat_prompt\n",
       "        )\n",
       "        retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
       "\n",
       "        chain.invoke({\"input\": \"...\"})\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/chatbot-langchain/lib/python3.10/site-packages/langchain/chains/retrieval.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?create_retrieval_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74a998c9-af21-4324-834e-85e7346574ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8616c65-a241-45ef-837d-4ac7d4ee27fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition can be achieved through various methods such as using Language Model (LLM) with simple prompting, providing task-specific instructions, or incorporating human inputs. These approaches help break down complex tasks into smaller components that are easier to manage and execute. By leveraging these techniques, agents can effectively navigate through intricate tasks and improve overall performance.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c57dceb-b9c3-4086-8994-ad0f4c7aba5e",
   "metadata": {},
   "source": [
    "So far we have manually updates the chat history, but we rather interested in automatically inserting and updating it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6ee99c-f40b-4a0d-abaf-a41c1b91b0b4",
   "metadata": {},
   "source": [
    "## Automatically updating chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "baeea3c6-4ccf-473e-b874-ddf4eea2287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14c7dc58-ec1d-444b-986d-7899769d35f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task decomposition is the process of breaking down complex tasks into smaller and simpler steps. This technique allows agents or models to handle difficult tasks by dividing them into more manageable sub-tasks. Different methods like Chain of Thought and Tree of Thoughts help in decomposing tasks effectively, guiding the model's thinking process towards successful completion.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5cda09a-98a1-46d1-b0f2-e2d2a5a8af78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition can be achieved through various methods, including using Language Model (LLM) with simple prompting, task-specific instructions, or human inputs. LLM can be prompted with questions like \"Steps for XYZ\" to break down tasks, while task-specific instructions like \"Write a story outline\" help in decomposing tasks based on specific goals. Human inputs also play a crucial role in guiding the decomposition process effectively.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97f60cfc-1336-40b0-a5ae-99481fada6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is Task Decomposition?\n",
      "\n",
      "AI: Task decomposition is the process of breaking down complex tasks into smaller and simpler steps. This technique allows agents or models to handle difficult tasks by dividing them into more manageable sub-tasks. Different methods like Chain of Thought and Tree of Thoughts help in decomposing tasks effectively, guiding the model's thinking process towards successful completion.\n",
      "\n",
      "User: What are common ways of doing it?\n",
      "\n",
      "AI: Task decomposition can be achieved through various methods, including using Language Model (LLM) with simple prompting, task-specific instructions, or human inputs. LLM can be prompted with questions like \"Steps for XYZ\" to break down tasks, while task-specific instructions like \"Write a story outline\" help in decomposing tasks based on specific goals. Human inputs also play a crucial role in guiding the decomposition process effectively.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in store[\"abc123\"].messages:\n",
    "    if isinstance(message, AIMessage):\n",
    "        prefix = \"AI\"\n",
    "    else:\n",
    "        prefix = \"User\"\n",
    "\n",
    "    print(f\"{prefix}: {message.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bac17b-fbce-4df6-8705-3836ced5f05e",
   "metadata": {},
   "source": [
    "![](img/LLM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef92664-9b10-4742-af90-cf881aaf7b49",
   "metadata": {},
   "source": [
    "History Aware Retriever: Usually a retriever just takes the query and works with it (it is not a LLM!). However, if the query is very generic such as \"how does it work?\" the retreiver will not understand it, which is why we need a LLM which translates it into an understandable query for the retriever. \n",
    "The retriever gives an answer that is formulated as a human friendly answer according to which system prompt was given to the LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864dca58-2748-4514-88d5-c64ece048958",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08faf44f-451f-4890-bf63-c8acd84d3a33",
   "metadata": {},
   "source": [
    "Agents generate the input to the retriever directly, without necessarily needing us to explicitly build in contextualization, as we did above;\n",
    "Agents can execute multiple retrieval steps in service of a query, or refrain from executing a retrieval step altogether (e.g., in response to a generic greeting from a user)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34b9025-6504-46a6-8b12-776d0aa62f4b",
   "metadata": {},
   "source": [
    "### Retrieval tool\n",
    "Agents can access \"tools\" and manage their execution. In this case, we will convert our retriever into a LangChain tool to be wielded by the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7897a7c5-c55d-4e41-a791-8fa682a64a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"blog_post_retriever\",\n",
    "    \"Searches and returns excerpts from the Autonomous Agents blog post.\",\n",
    ")\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f19865-a9b1-4487-be21-2ac0e0129fec",
   "metadata": {},
   "source": [
    "### Agent constructor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a4e1cea-4bd4-4e00-8c0a-c89c439b957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"blog_post_retriever\",\n",
    "    \"Searches and returns excerpts from the Autonomous Agents blog post.\",\n",
    ")\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd7e20d8-3490-46dc-8d25-f0d469cd6ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = create_react_agent(llm, tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c142ff0-e6ba-49dc-ab9e-a3fa31135252",
   "metadata": {},
   "source": [
    "LangGraph comes with built in persistence, so we don't need to use ChatMessageHistory! Rather, we can pass in a checkpointer to our LangGraph agent directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ffd49e85-8682-4126-8ad4-e0c72c13fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "agent_executor = create_react_agent(llm, tools, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f1a5044-60cb-4c05-bc02-7d2a31acb764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='Hello Bob! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 67, 'total_tokens': 78}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b7b12645-dbb4-4182-9f6d-c401aef56a67-0', usage_metadata={'input_tokens': 67, 'output_tokens': 11, 'total_tokens': 78})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Hi! I'm bob\")]}, config=config\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eac470-21cb-429b-bd8e-592fd585ebd8",
   "metadata": {},
   "source": [
    "Note that if we input a query that does not require a retrieval step, the agent does not execute one. Further, if we input a query that does require a retrieval step, the agent generates the input to the tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f96a27f3-f89c-4196-b619-0f3d027108ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Qm1nD2YFWpdkPZj0m8u9WlnL', 'function': {'arguments': '{\"query\":\"Task Decomposition\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 91, 'total_tokens': 110}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-893b21a6-3d01-406c-9de8-5517a8d0843d-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'Task Decomposition'}, 'id': 'call_Qm1nD2YFWpdkPZj0m8u9WlnL'}], usage_metadata={'input_tokens': 91, 'output_tokens': 19, 'total_tokens': 110})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:', name='blog_post_retriever', tool_call_id='call_Qm1nD2YFWpdkPZj0m8u9WlnL')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='Task decomposition is a technique used in complex tasks where the goal is to break down a larger task into smaller and more manageable steps. This approach helps in simplifying the problem-solving process by dividing it into subgoals or simpler tasks. One common method of task decomposition is the Chain of Thought (CoT) technique, which prompts the model to think step by step and decompose hard tasks into smaller steps.\\n\\nAnother advancement in task decomposition is the Tree of Thoughts, which explores multiple reasoning possibilities at each step by creating a tree structure of thought steps and generating multiple thoughts per step. This allows for a more in-depth analysis of the problem-solving process.\\n\\nTask decomposition can be achieved through various methods, such as using language models with simple prompting, task-specific instructions, or human inputs. Ultimately, the goal of task decomposition is to facilitate the planning and execution of complex tasks by breaking them down into more manageable components.', response_metadata={'token_usage': {'completion_tokens': 182, 'prompt_tokens': 611, 'total_tokens': 793}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e4acf3c3-6783-45a5-8e2a-b49b190bc6a4-0', usage_metadata={'input_tokens': 611, 'output_tokens': 182, 'total_tokens': 793})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Task Decomposition?\"\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]}, config=config\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0566508e-d1fa-45d6-94c0-fdd2e304c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_o83HfdbgDihPTs1UlA1TXZ5w', 'function': {'arguments': '{\"query\":\"Common ways of task decomposition\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 816, 'total_tokens': 837}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-01a00c8d-d4a9-42af-9d99-2cbbf34a75ff-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'Common ways of task decomposition'}, 'id': 'call_o83HfdbgDihPTs1UlA1TXZ5w'}], usage_metadata={'input_tokens': 816, 'output_tokens': 21, 'total_tokens': 837})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.', name='blog_post_retriever', tool_call_id='call_o83HfdbgDihPTs1UlA1TXZ5w')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='According to the blog post, common ways of task decomposition include the following methods:\\n\\n1. Using Language Models (LLM): Task decomposition can be done using language models with simple prompting. For example, prompting the model with instructions like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" helps in breaking down tasks into smaller steps.\\n\\n2. Task-Specific Instructions: Another approach is to provide task-specific instructions to the model. For instance, instructing the model to \"Write a story outline\" for writing a novel can guide the decomposition of the task into manageable components.\\n\\n3. Human Inputs: Task decomposition can also involve human inputs where individuals contribute to breaking down complex tasks into smaller, more manageable steps.\\n\\nThese methods help in simplifying complex tasks by breaking them down into smaller and more manageable components, facilitating better planning and execution.', response_metadata={'token_usage': {'completion_tokens': 172, 'prompt_tokens': 1361, 'total_tokens': 1533}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-46d5ba52-5dba-41ec-a3e3-76df9d1613e8-0', usage_metadata={'input_tokens': 1361, 'output_tokens': 172, 'total_tokens': 1533})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "query = \"What according to the blog post are common ways of doing it? redo the search\"\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]}, config=config\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1914dcd-a860-43fa-bf75-ebfd750ad138",
   "metadata": {},
   "source": [
    "The agent stripped unnecessary words like \"what\" and \"is\". The agent was able to infer that \"it\" in our query refers to \"task decomposition\", and generated a reasonable search query as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c77fedc9-475e-43bc-8084-513d8b9c66e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = create_react_agent(llm, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd038c-9abe-4d73-af33-bdfe138a1ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:chatbot-langchain]",
   "language": "python",
   "name": "conda-env-chatbot-langchain-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
